{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T02:40:06.254764Z",
     "iopub.status.busy": "2025-05-10T02:40:06.254063Z",
     "iopub.status.idle": "2025-05-10T02:40:06.260089Z",
     "shell.execute_reply": "2025-05-10T02:40:06.259095Z",
     "shell.execute_reply.started": "2025-05-10T02:40:06.254733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2, os, shutil, math\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T02:41:34.617456Z",
     "iopub.status.busy": "2025-05-10T02:41:34.616926Z",
     "iopub.status.idle": "2025-05-10T03:01:58.296979Z",
     "shell.execute_reply": "2025-05-10T03:01:58.296090Z",
     "shell.execute_reply.started": "2025-05-10T02:41:34.617423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_dataframes(sdir):\n",
    "    bad_images = []  # Giữ lại nếu có ảnh bị lỗi, nhưng không cần xử lý\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(sdir))  # Lấy danh sách các lớp bệnh (thư mục con)\n",
    "    \n",
    "    # Duyệt qua các thư mục bệnh\n",
    "    for klass in classes:\n",
    "        classpath = os.path.join(sdir, klass)\n",
    "        if not os.path.isdir(classpath):  # Nếu không phải thư mục thì bỏ qua\n",
    "            continue\n",
    "        flist = sorted(os.listdir(classpath))\n",
    "        desc = f'{klass:23s}'\n",
    "        \n",
    "        # Duyệt qua từng tệp trong thư mục\n",
    "        for f in tqdm(flist, ncols=110, desc=desc, unit='file', colour='blue'):\n",
    "            fpath = os.path.join(classpath, f)\n",
    "            try:\n",
    "                # Đọc ảnh mà không cần kiểm tra phần mở rộng\n",
    "                img = cv2.imread(fpath)\n",
    "                shape = img.shape  # Lấy kích thước ảnh\n",
    "                filepaths.append(fpath)\n",
    "                labels.append(klass)\n",
    "            except Exception as e:\n",
    "                bad_images.append(fpath)  # Lưu lại ảnh bị lỗi\n",
    "                print(f'Defective image file: {fpath}, Error: {e}')\n",
    "    \n",
    "    # Tạo DataFrame từ danh sách filepaths và labels\n",
    "    Fseries = pd.Series(filepaths, name='filepaths')\n",
    "    Lseries = pd.Series(labels, name='labels')\n",
    "    df = pd.concat([Fseries, Lseries], axis=1)\n",
    "    \n",
    "    # Chia dữ liệu thành train, validation và test (80-10-10)\n",
    "    train_df, dummy_df = train_test_split(df, train_size=.8, shuffle=True, random_state=123, stratify=df['labels'])\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify=dummy_df['labels'])\n",
    "    \n",
    "    # Tính toán một số thông số về dữ liệu\n",
    "    classes = sorted(train_df['labels'].unique())\n",
    "    class_count = len(classes)\n",
    "    sample_df = train_df.sample(n=50, replace=False)\n",
    "    \n",
    "    ht, wt, count = 0, 0, 0\n",
    "    for i in range(len(sample_df)):\n",
    "        fpath = sample_df['filepaths'].iloc[i]\n",
    "        try:\n",
    "            img = cv2.imread(fpath)\n",
    "            h, w = img.shape[:2]\n",
    "            ht += h\n",
    "            wt += w\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    if count > 0:\n",
    "        ave_height = ht // count\n",
    "        ave_width = wt // count\n",
    "        aspect_ratio = ave_height / ave_width\n",
    "    else:\n",
    "        ave_height, ave_width, aspect_ratio = 0, 0, 0\n",
    "    \n",
    "    # Hiển thị thông tin thống kê\n",
    "    print(f'Number of classes in processed dataset: {class_count}')\n",
    "    counts = list(train_df['labels'].value_counts())\n",
    "    print(f'Max files in any class in train_df: {max(counts)}, Min files in any class: {min(counts)}')\n",
    "    print(f'Train dataset length: {len(train_df)}, Test dataset length: {len(test_df)}, Validation dataset length: {len(valid_df)}')\n",
    "    print(f'Average image height: {ave_height}, Average image width: {ave_width}, Aspect ratio (height/width): {aspect_ratio}')\n",
    "    \n",
    "    return train_df, test_df, valid_df, classes, class_count\n",
    "\n",
    "# Đọc và chia dữ liệu\n",
    "sdir = 'D:/2011/folder1'  # Đường dẫn tới thư mục ảnh\n",
    "train_df, test_df, valid_df, classes, class_count = make_dataframes(sdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:02:37.860853Z",
     "iopub.status.busy": "2025-05-10T03:02:37.860154Z",
     "iopub.status.idle": "2025-05-10T03:08:06.801824Z",
     "shell.execute_reply": "2025-05-10T03:08:06.800847Z",
     "shell.execute_reply.started": "2025-05-10T03:02:37.860819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n=2500\n",
    "batch_size = 32\n",
    "working_dir=r'./'\n",
    "img_size=(300, 300)\n",
    "epochs = 20 \n",
    "input_shape = (300, 300, 3)\n",
    "\n",
    "\n",
    "def balance(df, n, working_dir, img_size):\n",
    "    df = df.copy()\n",
    "    print('Initial length of dataframe is ', len(df))\n",
    "    aug_dir = os.path.join(working_dir, 'aug')\n",
    "    if os.path.isdir(aug_dir):\n",
    "        shutil.rmtree(aug_dir)\n",
    "    os.mkdir(aug_dir)\n",
    "\n",
    "    for label in df['labels'].unique():\n",
    "        dir_path = os.path.join(aug_dir, label)\n",
    "        os.mkdir(dir_path)\n",
    "    total = 0\n",
    "    gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,  # Thêm shear\n",
    "    fill_mode='reflect',  # Thay đổi từ 'nearest' sang 'reflect'\n",
    "    brightness_range=[0.8, 1.2],  # Thêm brightness augmentation\n",
    "    channel_shift_range=50.0,  # Thêm channel shift\n",
    "    validation_split=0.2\n",
    "    )\n",
    "    groups = df.groupby('labels')\n",
    "    for label in df['labels'].unique():\n",
    "        group = groups.get_group(label)\n",
    "        sample_count = len(group)\n",
    "        if sample_count < n:\n",
    "            aug_img_count = 0\n",
    "            delta = n - sample_count\n",
    "            target_dir = os.path.join(aug_dir, label)\n",
    "            msg = '{0:40s} for class {1:^30s} creating {2:^5s} augmented images'.format(' ', label, str(delta))\n",
    "            print(msg, '\\r', end='')  # prints over on the same line\n",
    "            aug_gen = gen.flow_from_dataframe(group, x_col='filepaths', y_col=None, target_size=img_size,\n",
    "                                              class_mode=None, batch_size=batch_size, shuffle=False,\n",
    "                                              save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',\n",
    "                                              save_format='jpg')\n",
    "            while aug_img_count < delta:\n",
    "                images = next(aug_gen)\n",
    "                aug_img_count += len(images)\n",
    "            total += aug_img_count\n",
    "    print('Total Augmented images created= ', total)\n",
    "    aug_fpaths, aug_labels = [], []\n",
    "    classlist = os.listdir(aug_dir)\n",
    "    for target in classlist:\n",
    "        classpath = os.path.join(aug_dir, target)\n",
    "        flist = os.listdir(classpath)\n",
    "        for f in flist:\n",
    "            fpath = os.path.join(classpath, f)\n",
    "            aug_fpaths.append(fpath)\n",
    "            aug_labels.append(target)\n",
    "    Fseries = pd.Series(aug_fpaths, name='filepaths')\n",
    "    Lseries = pd.Series(aug_labels, name='labels')\n",
    "    aug_df = pd.concat([Fseries, Lseries], axis=1)\n",
    "    df = pd.concat([df, aug_df], axis=0).reset_index(drop=True)\n",
    "    print('Length of augmented dataframe is ', len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = balance(train_df, n, working_dir, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:08:17.206478Z",
     "iopub.status.busy": "2025-05-10T03:08:17.206118Z",
     "iopub.status.idle": "2025-05-10T03:08:19.748890Z",
     "shell.execute_reply": "2025-05-10T03:08:19.747818Z",
     "shell.execute_reply.started": "2025-05-10T03:08:17.206442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "working_dir=r'./'\n",
    "img_size=(300, 300)\n",
    "epochs = 20 \n",
    "input_shape = (300, 300, 3)\n",
    "\n",
    "def make_gens(batch_size, train_df, test_df, valid_df, img_size):\n",
    "    trgen = ImageDataGenerator(horizontal_flip=True)\n",
    "    t_and_v_gen = ImageDataGenerator()\n",
    "    msg = '{0:70s} for train generator'.format(' ')\n",
    "    print(msg, '\\r', end='')\n",
    "    train_ds = trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels',\n",
    "                                         target_size=img_size, class_mode='categorical',\n",
    "                                         color_mode='rgb', batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    msg = '{0:70s} for valid generator'.format(' ')\n",
    "    print(msg, '\\r', end='')\n",
    "    valid_ds = t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels',\n",
    "                                         target_size=img_size, class_mode='categorical',\n",
    "                                         color_mode='rgb', batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_len = len(test_df)\n",
    "    test_batch_size = sorted([int(test_len / n) for n in range(1, test_len + 1)\n",
    "                              if test_len % n == 0 and test_len / n<=80], reverse=True)[0]\n",
    "    test_steps = int(test_len / test_batch_size)\n",
    "    msg = '{0:70s} for test generator'.format(' ')\n",
    "    print(msg, '\\r', end='')\n",
    "    test_ds = t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels',\n",
    "                                               target_size=img_size, class_mode='categorical',\n",
    "                                               color_mode='rgb', batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = list(train_ds.class_indices.keys())\n",
    "    class_count = len(classes)\n",
    "    print('test batch size: ', test_batch_size, 'test steps: ', test_steps, 'number of classes : ', class_count)\n",
    "\n",
    "    return train_ds, test_ds, valid_ds\n",
    "\n",
    "train_ds, test_ds, valid_ds = make_gens(batch_size, train_df, test_df, valid_df, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:08:19.751361Z",
     "iopub.status.busy": "2025-05-10T03:08:19.750989Z",
     "iopub.status.idle": "2025-05-10T03:08:19.765593Z",
     "shell.execute_reply": "2025-05-10T03:08:19.764477Z",
     "shell.execute_reply.started": "2025-05-10T03:08:19.751325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def h_sigmoid(x):\n",
    "    res = tf.nn.relu6(x + 3) / 6\n",
    "    return res\n",
    "\n",
    "def h_swish(x):\n",
    "    res = x * h_sigmoid(x)\n",
    "    return res\n",
    "\n",
    "\n",
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_channels, r=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.P1 = GlobalAveragePooling2D()\n",
    "        self.D1 = Dense(input_channels // r)\n",
    "        self.A1 = Activation('relu')\n",
    "        self.D2 = Dense(input_channels)\n",
    "\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        branch = self.P1(inputs)\n",
    "        branch = self.D1(branch)\n",
    "        branch = self.A1(branch)\n",
    "        branch = self.D2(branch)\n",
    "        branch = h_sigmoid(branch)\n",
    "        branch = tf.expand_dims(input=branch, axis=1)\n",
    "        branch = tf.expand_dims(input=branch, axis=1)\n",
    "        y = inputs * branch\n",
    "        return y\n",
    "\n",
    "\n",
    "class BottleNeck(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_size, exp_size, out_size, s, is_se_existing, NL, k):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.stride = s\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.is_se_existing = is_se_existing\n",
    "        self.NL = NL\n",
    "\n",
    "        self.C1 = Conv2D(filters=exp_size, kernel_size=(1, 1), strides=1, padding='same')\n",
    "        self.B1 = BatchNormalization()\n",
    "\n",
    "        self.DWC1 = DepthwiseConv2D(kernel_size=(k, k), strides=s, padding='same')\n",
    "        self.B2 = BatchNormalization()\n",
    "\n",
    "        self.se = SEBlock(input_channels=exp_size)\n",
    "        self.C2 = Conv2D(filters=out_size, kernel_size=(1, 1), strides=1, padding='same')\n",
    "        self.B3 = BatchNormalization()\n",
    "        self.linear = Activation(tf.keras.activations.linear)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        x = self.C1(inputs)\n",
    "        x = self.B1(x, training=training)\n",
    "\n",
    "        if self.NL == 'HS':\n",
    "            x = h_swish(x)\n",
    "        elif self.NL == 'RE':\n",
    "            x = tf.nn.relu6(x)\n",
    "\n",
    "        x = self.DWC1(x)\n",
    "        x = self.B2(x, training=training)\n",
    "\n",
    "        if self.NL == 'HS':\n",
    "            x = h_swish(x)\n",
    "        elif self.NL == 'RE':\n",
    "            x = tf.nn.relu6(x)\n",
    "\n",
    "        if self.is_se_existing:\n",
    "            x = self.se(x)\n",
    "\n",
    "        x = self.C2(x)\n",
    "        x = self.B3(x)\n",
    "        y = self.linear(x)\n",
    "\n",
    "        if self.stride == 1 and self.in_size == self.out_size:\n",
    "            y = add([y, inputs])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:08:21.536416Z",
     "iopub.status.busy": "2025-05-10T03:08:21.536061Z",
     "iopub.status.idle": "2025-05-10T03:08:21.555808Z",
     "shell.execute_reply": "2025-05-10T03:08:21.554881Z",
     "shell.execute_reply.started": "2025-05-10T03:08:21.536375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MobileNetV3Small(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MobileNetV3Small, self).__init__()\n",
    "        self.C1 = Conv2D(filters=16, kernel_size=(3, 3), strides=2, padding=\"same\", input_shape=input_shape)\n",
    "        self.B1 = BatchNormalization()\n",
    "        self.bneck1 = BottleNeck(in_size=16, exp_size=16, out_size=16, s=2, is_se_existing=True, NL=\"RE\", k=3)\n",
    "        self.bneck2 = BottleNeck(in_size=16, exp_size=72, out_size=24, s=2, is_se_existing=False, NL=\"RE\", k=3)\n",
    "        self.bneck3 = BottleNeck(in_size=24, exp_size=88, out_size=24, s=1, is_se_existing=False, NL=\"RE\", k=3)\n",
    "        self.bneck4 = BottleNeck(in_size=24, exp_size=96, out_size=40, s=2, is_se_existing=True, NL=\"HS\", k=5)\n",
    "        self.bneck5 = BottleNeck(in_size=40, exp_size=240, out_size=40, s=1, is_se_existing=True, NL=\"HS\", k=5)\n",
    "        self.bneck6 = BottleNeck(in_size=40, exp_size=240, out_size=40, s=1, is_se_existing=True, NL=\"HS\", k=5)\n",
    "        self.bneck7 = BottleNeck(in_size=40, exp_size=120, out_size=48, s=1, is_se_existing=True, NL=\"HS\", k=5)\n",
    "        self.bneck8 = BottleNeck(in_size=48, exp_size=144, out_size=48, s=1, is_se_existing=True, NL=\"HS\", k=5)\n",
    "        self.bneck9 = BottleNeck(in_size=48, exp_size=288, out_size=96, s=2, is_se_existing=True, NL=\"HS\", k=5)\n",
    "        self.bneck10 = BottleNeck(in_size=96, exp_size=576, out_size=96, s=1, is_se_existing=True, NL=\"HS\", k=5)\n",
    "        self.bneck11 = BottleNeck(in_size=96, exp_size=576, out_size=96, s=1, is_se_existing=True, NL=\"HS\", k=5)\n",
    "\n",
    "        self.C2 = Conv2D(filters=576,\n",
    "                            kernel_size=(1, 1),\n",
    "                            strides=1,\n",
    "                            padding=\"same\")\n",
    "        self.B2 = BatchNormalization()\n",
    "        self.P1 = AveragePooling2D(pool_size=(7, 7), strides=1)\n",
    "\n",
    "        self.C3 = Conv2D(filters=1280, kernel_size=(1, 1), strides=1, padding=\"same\")\n",
    "\n",
    "        self.C4 = Conv2D(filters=1024, kernel_size=(1, 1), strides=1, padding=\"same\", activation='softmax')\n",
    "\n",
    "        self.P2 = GlobalAveragePooling2D()\n",
    "        \n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.C1(inputs)\n",
    "        x = self.B1(x, training=training)\n",
    "        x = h_swish(x)\n",
    "\n",
    "        x = self.bneck1(x, training=training)\n",
    "        x = self.bneck2(x, training=training)\n",
    "        x = self.bneck3(x, training=training)\n",
    "        x = self.bneck4(x, training=training)\n",
    "        x = self.bneck5(x, training=training)\n",
    "        x = self.bneck6(x, training=training)\n",
    "        x = self.bneck7(x, training=training)\n",
    "        x = self.bneck8(x, training=training)\n",
    "        x = self.bneck9(x, training=training)\n",
    "        x = self.bneck10(x, training=training)\n",
    "        x = self.bneck11(x, training=training)\n",
    "\n",
    "        x = self.C2(x)\n",
    "        x = self.B2(x, training=training)\n",
    "        x = h_swish(x)\n",
    "        x = self.P1(x)\n",
    "        x = self.C3(x)\n",
    "        x = h_swish(x)\n",
    "        x = self.C4(x)\n",
    "        y = self.P2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:08:26.548065Z",
     "iopub.status.busy": "2025-05-10T03:08:26.547368Z",
     "iopub.status.idle": "2025-05-10T03:08:26.558778Z",
     "shell.execute_reply": "2025-05-10T03:08:26.557762Z",
     "shell.execute_reply.started": "2025-05-10T03:08:26.548030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def GELU(x):\n",
    "    res = 0.5 * x * (1 + tf.nn.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * (x ** 3))))\n",
    "    return res\n",
    "\n",
    "\n",
    "# 定义残差MLP结构块\n",
    "class ResMLPBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, residual_path):\n",
    "        super(ResMLPBlock, self).__init__()\n",
    "        self.residual_path = residual_path\n",
    "        self.D1 = Dense(units, activation='relu')\n",
    "        self.D2 = Dense(units, activation='relu')\n",
    "\n",
    "        if self.residual_path:\n",
    "            self.D3 = Dense(units)\n",
    "            self.D4 = Dense(units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        residual = inputs\n",
    "\n",
    "        x = self.D1(inputs)\n",
    "        y = self.D2(x)\n",
    "\n",
    "        if self.residual_path:\n",
    "            residual = self.D3(inputs)\n",
    "            residual = GELU(residual)\n",
    "            residual = self.D4(residual)\n",
    "            residual = GELU(residual)\n",
    "\n",
    "        output = y + residual\n",
    "        return output\n",
    "\n",
    "\n",
    "# ResMLP网络结构\n",
    "class ResMLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, initial_filters, block_list, num_classes):\n",
    "        super(ResMLP, self).__init__()\n",
    "        self.initial_filters = initial_filters\n",
    "        self.block_list = block_list\n",
    "\n",
    "        self.D1 = Dense(self.initial_filters, activation='relu')\n",
    "        self.B1 = BatchNormalization()\n",
    "\n",
    "        self.blocks = tf.keras.models.Sequential()\n",
    "        for block_id in range(len(block_list)):\n",
    "            for layer_id in range(block_list[block_id]):\n",
    "                if block_id != 0 and layer_id == 0:\n",
    "                    block = ResMLPBlock(units=self.initial_filters, residual_path=True)\n",
    "                else:\n",
    "                    block = ResMLPBlock(units=self.initial_filters, residual_path=False)\n",
    "                self.blocks.add(block)\n",
    "            self.initial_filters *= 2\n",
    "\n",
    "        self.D2 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.D1(inputs)\n",
    "        x = self.B1(x)\n",
    "        x = self.blocks(x)\n",
    "        y = self.D2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:08:30.739818Z",
     "iopub.status.busy": "2025-05-10T03:08:30.739132Z",
     "iopub.status.idle": "2025-05-10T03:08:30.745232Z",
     "shell.execute_reply": "2025-05-10T03:08:30.744114Z",
     "shell.execute_reply.started": "2025-05-10T03:08:30.739786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_shape, initial_filters, block_list, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = MobileNetV3Small(input_shape=input_shape)\n",
    "        self.layer2 = ResMLP(initial_filters=initial_filters, block_list=block_list, num_classes=num_classes)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.layer1(x)\n",
    "        y = self.layer2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:08:33.568419Z",
     "iopub.status.busy": "2025-05-10T03:08:33.567592Z",
     "iopub.status.idle": "2025-05-10T04:45:48.903357Z",
     "shell.execute_reply": "2025-05-10T04:45:48.902501Z",
     "shell.execute_reply.started": "2025-05-10T03:08:33.568371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "net = NeuralNetwork(input_shape=input_shape, initial_filters=32, block_list=[2, 2, 2], num_classes=6)\n",
    "\n",
    "net.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['AUC', 'accuracy'])  # Thêm 'accuracy' vào metrics\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = net.fit(train_ds, \n",
    "                      epochs=epochs, # epochs = 50 như bạn định nghĩa ở cell 25\n",
    "                      batch_size=batch_size, \n",
    "                      validation_data=valid_ds,\n",
    "                      callbacks=[reduce_lr, early_stop]) # Thêm callbacks\n",
    "\n",
    "net.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:45:48.905105Z",
     "iopub.status.busy": "2025-05-10T04:45:48.904799Z",
     "iopub.status.idle": "2025-05-10T04:45:49.308409Z",
     "shell.execute_reply": "2025-05-10T04:45:49.307460Z",
     "shell.execute_reply.started": "2025-05-10T04:45:48.905079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_auc_loss(history, epochs):\n",
    "    # Lấy dữ liệu từ history\n",
    "    tloss = history.history['loss']  # Training loss\n",
    "    vloss = history.history['val_loss']  # Validation loss\n",
    "\n",
    "    # Kiểm tra và in ra số lượng phần tử trong các danh sách\n",
    "    print(f\"Number of epochs: {epochs}\")\n",
    "    print(f\"Training loss data points: {len(tloss)}\")\n",
    "    print(f\"Validation loss data points: {len(vloss)}\")\n",
    "\n",
    "    # Kiểm tra rằng số phần tử trong các list khớp với số epoch\n",
    "    if len(tloss) != epochs or len(vloss) != epochs:\n",
    "        print(\"Mismatch in number of epochs and data points. Adjusting to match.\")\n",
    "        epochs = min(len(tloss), len(vloss))  # Điều chỉnh lại epochs nếu có sự khác biệt\n",
    "\n",
    "    Epochs = range(1, epochs + 1)  # Tạo list Epochs có độ dài giống với loss\n",
    "\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "\n",
    "    # Vẽ biểu đồ Training Loss và Validation Loss\n",
    "    axes[0].plot(Epochs, tloss[:epochs], 'r', label='Training loss')  # Cắt phần tử phù hợp với số epoch\n",
    "    axes[0].plot(Epochs, vloss[:epochs], 'g', label='Validation loss')  # Cắt phần tử phù hợp với số epoch\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Vẽ biểu đồ Accuracy nếu có\n",
    "    if 'accuracy' in history.history:\n",
    "        tacc = history.history['accuracy']\n",
    "        vacc = history.history['val_accuracy']\n",
    "        axes[1].plot(Epochs, tacc[:epochs], 'r', label='Training accuracy')  # Cắt phần tử phù hợp với số epoch\n",
    "        axes[1].plot(Epochs, vacc[:epochs], 'g', label='Validation accuracy')  # Cắt phần tử phù hợp với số epoch\n",
    "        axes[1].set_title('Training and Validation accuracy')\n",
    "        axes[1].set_xlabel('Epochs')\n",
    "        axes[1].set_ylabel('accuracy')\n",
    "        axes[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sử dụng hàm với số epochs bạn đã dùng để huấn luyện\n",
    "plot_auc_loss(history, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:45:49.309867Z",
     "iopub.status.busy": "2025-05-10T04:45:49.309547Z",
     "iopub.status.idle": "2025-05-10T04:47:58.237715Z",
     "shell.execute_reply": "2025-05-10T04:47:58.236848Z",
     "shell.execute_reply.started": "2025-05-10T04:45:49.309840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predictor(test_ds):\n",
    "    y_pred, error_list, error_pred_list = [], [], []\n",
    "    y_true = test_ds.labels\n",
    "    classes = list(test_ds.class_indices.keys())\n",
    "    class_count = len(classes)\n",
    "    errors = 0\n",
    "    preds = tf.argmax(net.predict(test_ds), axis=1)\n",
    "    tests = len(preds)\n",
    "    for i in range(tests):\n",
    "        pred_index = preds[i]\n",
    "        true_index = test_ds.labels[i]\n",
    "        if pred_index != true_index:\n",
    "            errors += 1\n",
    "            file = test_ds.filenames[i]\n",
    "            error_list.append(file)\n",
    "            error_classes = classes[pred_index]\n",
    "            error_pred_list.append(error_classes)\n",
    "        y_pred.append(pred_index)\n",
    "\n",
    "    acc = (1 - errors / tests) * 100\n",
    "    msg = f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}%'\n",
    "    print(msg)\n",
    "    ypred = np.array(y_pred)\n",
    "    ytrue = np.array(y_true)\n",
    "    f1score = f1_score(ytrue, ypred, average='weighted') * 100\n",
    "\n",
    "    if class_count <= 30:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
    "        plt.xticks(np.arange(class_count) + .5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count) + .5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    clr = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "\n",
    "    return errors, tests, error_list, error_pred_list, f1score\n",
    "\n",
    "errors, tests, error_list, error_pred_list, f1score = predictor(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:47:58.240950Z",
     "iopub.status.busy": "2025-05-10T04:47:58.240566Z",
     "iopub.status.idle": "2025-05-10T04:47:58.394420Z",
     "shell.execute_reply": "2025-05-10T04:47:58.393585Z",
     "shell.execute_reply.started": "2025-05-10T04:47:58.240924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Kết hợp tất cả các DataFrame train, test, và validation\n",
    "all_data = pd.concat([train_df, test_df, valid_df])\n",
    "\n",
    "# Đếm số lượng mẫu cho mỗi lớp trong toàn bộ bộ dữ liệu\n",
    "class_counts_all = all_data['labels'].value_counts()\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts_all.plot(kind='bar', color='skyblue')\n",
    "plt.title('Number of Samples per Class (All datasets)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu model dưới dạng SavedModel\n",
    "saved_model_path = './model/mobilenetV3_resmlp'\n",
    "net.save(saved_model_path, save_format='tf')\n",
    "print(f\"Model đã được lưu dưới dạng SavedModel tại: {saved_model_path}\")\n",
    "\n",
    "# Lưu model dưới dạng TFLite\n",
    "tflite_save_path = './model/mobilenetV3_resmlp.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(net)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(tflite_save_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"Model đã được lưu dưới dạng TFLite tại: {tflite_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Đường dẫn đến SavedModel đã lưu\n",
    "saved_model_path = './model/mobilenetV3_resmlp'\n",
    "\n",
    "# Đường dẫn thư mục để lưu model TensorFlow.js\n",
    "tfjs_model_dir = './model/modeljs'\n",
    "\n",
    "# Chuyển đổi model sang TensorFlow.js\n",
    "result = subprocess.run([\n",
    "    'tensorflowjs_converter',\n",
    "    '--input_format', 'tf_saved_model',\n",
    "    '--output_format', 'tfjs_graph_model',\n",
    "    '--saved_model_tags', 'serve',\n",
    "    saved_model_path,\n",
    "    tfjs_model_dir\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Lỗi trong quá trình chuyển đổi: {result.stderr}\")\n",
    "else:\n",
    "    print(f\"Model đã được chuyển đổi sang TensorFlow.js và lưu tại: {tfjs_model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2464177,
     "sourceId": 4176149,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7335914,
     "sourceId": 11688016,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
