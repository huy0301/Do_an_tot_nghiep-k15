{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T02:40:06.254764Z",
     "iopub.status.busy": "2025-05-10T02:40:06.254063Z",
     "iopub.status.idle": "2025-05-10T02:40:06.260089Z",
     "shell.execute_reply": "2025-05-10T02:40:06.259095Z",
     "shell.execute_reply.started": "2025-05-10T02:40:06.254733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2, os, shutil, math\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T02:41:34.617456Z",
     "iopub.status.busy": "2025-05-10T02:41:34.616926Z",
     "iopub.status.idle": "2025-05-10T03:01:58.296979Z",
     "shell.execute_reply": "2025-05-10T03:01:58.296090Z",
     "shell.execute_reply.started": "2025-05-10T02:41:34.617423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_dataframes(sdir):\n",
    "    bad_images = []  # Giữ lại nếu có ảnh bị lỗi, nhưng không cần xử lý\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(sdir))  # Lấy danh sách các lớp bệnh (thư mục con)\n",
    "    \n",
    "    # Duyệt qua các thư mục bệnh\n",
    "    for klass in classes:\n",
    "        classpath = os.path.join(sdir, klass)\n",
    "        if not os.path.isdir(classpath):  # Nếu không phải thư mục thì bỏ qua\n",
    "            continue\n",
    "        flist = sorted(os.listdir(classpath))\n",
    "        desc = f'{klass:23s}'\n",
    "        \n",
    "        # Duyệt qua từng tệp trong thư mục\n",
    "        for f in tqdm(flist, ncols=110, desc=desc, unit='file', colour='blue'):\n",
    "            fpath = os.path.join(classpath, f)\n",
    "            try:\n",
    "                # Đọc ảnh mà không cần kiểm tra phần mở rộng\n",
    "                img = cv2.imread(fpath)\n",
    "                shape = img.shape  # Lấy kích thước ảnh\n",
    "                filepaths.append(fpath)\n",
    "                labels.append(klass)\n",
    "            except Exception as e:\n",
    "                bad_images.append(fpath)  # Lưu lại ảnh bị lỗi\n",
    "                print(f'Defective image file: {fpath}, Error: {e}')\n",
    "    \n",
    "    # Tạo DataFrame từ danh sách filepaths và labels\n",
    "    Fseries = pd.Series(filepaths, name='filepaths')\n",
    "    Lseries = pd.Series(labels, name='labels')\n",
    "    df = pd.concat([Fseries, Lseries], axis=1)\n",
    "    \n",
    "    # Chia dữ liệu thành train, validation và test (80-10-10)\n",
    "    train_df, dummy_df = train_test_split(df, train_size=.8, shuffle=True, random_state=123, stratify=df['labels'])\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify=dummy_df['labels'])\n",
    "    \n",
    "    # Tính toán một số thông số về dữ liệu\n",
    "    classes = sorted(train_df['labels'].unique())\n",
    "    class_count = len(classes)\n",
    "    sample_df = train_df.sample(n=50, replace=False)\n",
    "    \n",
    "    ht, wt, count = 0, 0, 0\n",
    "    for i in range(len(sample_df)):\n",
    "        fpath = sample_df['filepaths'].iloc[i]\n",
    "        try:\n",
    "            img = cv2.imread(fpath)\n",
    "            h, w = img.shape[:2]\n",
    "            ht += h\n",
    "            wt += w\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    if count > 0:\n",
    "        ave_height = ht // count\n",
    "        ave_width = wt // count\n",
    "        aspect_ratio = ave_height / ave_width\n",
    "    else:\n",
    "        ave_height, ave_width, aspect_ratio = 0, 0, 0\n",
    "    \n",
    "    # Hiển thị thông tin thống kê\n",
    "    print(f'Number of classes in processed dataset: {class_count}')\n",
    "    counts = list(train_df['labels'].value_counts())\n",
    "    print(f'Max files in any class in train_df: {max(counts)}, Min files in any class: {min(counts)}')\n",
    "    print(f'Train dataset length: {len(train_df)}, Test dataset length: {len(test_df)}, Validation dataset length: {len(valid_df)}')\n",
    "    print(f'Average image height: {ave_height}, Average image width: {ave_width}, Aspect ratio (height/width): {aspect_ratio}')\n",
    "    \n",
    "    return train_df, test_df, valid_df, classes, class_count\n",
    "\n",
    "# Đọc và chia dữ liệu\n",
    "sdir = 'D:/2011/folder1'  # Đường dẫn tới thư mục ảnh\n",
    "train_df, test_df, valid_df, classes, class_count = make_dataframes(sdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:02:37.860853Z",
     "iopub.status.busy": "2025-05-10T03:02:37.860154Z",
     "iopub.status.idle": "2025-05-10T03:08:06.801824Z",
     "shell.execute_reply": "2025-05-10T03:08:06.800847Z",
     "shell.execute_reply.started": "2025-05-10T03:02:37.860819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n=2500\n",
    "batch_size = 32\n",
    "working_dir=r'./'\n",
    "img_size=(300, 300)\n",
    "epochs = 50\n",
    "input_shape = (300, 300, 3)\n",
    "\n",
    "\n",
    "def balance(df, n, working_dir, img_size):\n",
    "    df = df.copy()\n",
    "    print('Initial length of dataframe is ', len(df))\n",
    "    aug_dir = os.path.join(working_dir, 'aug')\n",
    "    if os.path.isdir(aug_dir):\n",
    "        shutil.rmtree(aug_dir)\n",
    "    os.mkdir(aug_dir)\n",
    "\n",
    "    for label in df['labels'].unique():\n",
    "        dir_path = os.path.join(aug_dir, label)\n",
    "        os.mkdir(dir_path)\n",
    "    total = 0\n",
    "    gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,  # Thêm shear\n",
    "    fill_mode='reflect',  # Thay đổi từ 'nearest' sang 'reflect'\n",
    "    brightness_range=[0.8, 1.2],  # Thêm brightness augmentation\n",
    "    channel_shift_range=50.0,  # Thêm channel shift\n",
    "    validation_split=0.2\n",
    "    )\n",
    "    groups = df.groupby('labels')\n",
    "    for label in df['labels'].unique():\n",
    "        group = groups.get_group(label)\n",
    "        sample_count = len(group)\n",
    "        if sample_count < n:\n",
    "            aug_img_count = 0\n",
    "            delta = n - sample_count\n",
    "            target_dir = os.path.join(aug_dir, label)\n",
    "            msg = '{0:40s} for class {1:^30s} creating {2:^5s} augmented images'.format(' ', label, str(delta))\n",
    "            print(msg, '\\r', end='')  # prints over on the same line\n",
    "            aug_gen = gen.flow_from_dataframe(group, x_col='filepaths', y_col=None, target_size=img_size,\n",
    "                                              class_mode=None, batch_size=batch_size, shuffle=False,\n",
    "                                              save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',\n",
    "                                              save_format='jpg')\n",
    "            while aug_img_count < delta:\n",
    "                images = next(aug_gen)\n",
    "                aug_img_count += len(images)\n",
    "            total += aug_img_count\n",
    "    print('Total Augmented images created= ', total)\n",
    "    aug_fpaths, aug_labels = [], []\n",
    "    classlist = os.listdir(aug_dir)\n",
    "    for target in classlist:\n",
    "        classpath = os.path.join(aug_dir, target)\n",
    "        flist = os.listdir(classpath)\n",
    "        for f in flist:\n",
    "            fpath = os.path.join(classpath, f)\n",
    "            aug_fpaths.append(fpath)\n",
    "            aug_labels.append(target)\n",
    "    Fseries = pd.Series(aug_fpaths, name='filepaths')\n",
    "    Lseries = pd.Series(aug_labels, name='labels')\n",
    "    aug_df = pd.concat([Fseries, Lseries], axis=1)\n",
    "    df = pd.concat([df, aug_df], axis=0).reset_index(drop=True)\n",
    "    print('Length of augmented dataframe is ', len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = balance(train_df, n, working_dir, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:08:17.206478Z",
     "iopub.status.busy": "2025-05-10T03:08:17.206118Z",
     "iopub.status.idle": "2025-05-10T03:08:19.748890Z",
     "shell.execute_reply": "2025-05-10T03:08:19.747818Z",
     "shell.execute_reply.started": "2025-05-10T03:08:17.206442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_gens(batch_size, train_df, test_df, valid_df, img_size):\n",
    "    trgen = ImageDataGenerator(horizontal_flip=True)\n",
    "    t_and_v_gen = ImageDataGenerator()\n",
    "    msg = '{0:70s} for train generator'.format(' ')\n",
    "    print(msg, '\\r', end='')\n",
    "    train_ds = trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels',\n",
    "                                         target_size=img_size, class_mode='categorical',\n",
    "                                         color_mode='rgb', batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    msg = '{0:70s} for valid generator'.format(' ')\n",
    "    print(msg, '\\r', end='')\n",
    "    valid_ds = t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels',\n",
    "                                         target_size=img_size, class_mode='categorical',\n",
    "                                         color_mode='rgb', batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_len = len(test_df)\n",
    "    test_batch_size = sorted([int(test_len / n) for n in range(1, test_len + 1)\n",
    "                              if test_len % n == 0 and test_len / n<=80], reverse=True)[0]\n",
    "    test_steps = int(test_len / test_batch_size)\n",
    "    msg = '{0:70s} for test generator'.format(' ')\n",
    "    print(msg, '\\r', end='')\n",
    "    test_ds = t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels',\n",
    "                                               target_size=img_size, class_mode='categorical',\n",
    "                                               color_mode='rgb', batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = list(train_ds.class_indices.keys())\n",
    "    class_count = len(classes)\n",
    "    print('test batch size: ', test_batch_size, 'test steps: ', test_steps, 'number of classes : ', class_count)\n",
    "\n",
    "    return train_ds, test_ds, valid_ds\n",
    "\n",
    "train_ds, test_ds, valid_ds = make_gens(batch_size, train_df, test_df, valid_df, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tải InceptionV3 pre-trained model\n",
    "base_model_inception = InceptionV3(\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=input_shape, \n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "# 2. Đóng băng một phần các lớp của base_model\n",
    "# Chỉ mở khóa các lớp cuối để fine-tuning\n",
    "for layer in base_model_inception.layers[:-50]:  # Giữ đóng băng các lớp đầu\n",
    "    layer.trainable = False\n",
    "for layer in base_model_inception.layers[-50:]:  # Mở khóa các lớp cuối để fine-tuning\n",
    "    layer.trainable = True\n",
    "\n",
    "# 3. Thêm các lớp tùy chỉnh với kiến trúc mạnh hơn\n",
    "x_inception = GlobalAveragePooling2D(name='inception_avg_pool')(base_model_inception.output)\n",
    "x_inception = BatchNormalization()(x_inception)\n",
    "x_inception = Dense(1024, activation=\"relu\", name='inception_fc1')(x_inception)\n",
    "x_inception = BatchNormalization()(x_inception)\n",
    "x_inception = Dropout(0.5, name='inception_dropout1')(x_inception)  # Tăng dropout để giảm overfitting\n",
    "x_inception = Dense(512, activation=\"relu\", name='inception_fc2')(x_inception)\n",
    "x_inception = BatchNormalization()(x_inception)\n",
    "x_inception = Dropout(0.3, name='inception_dropout2')(x_inception)\n",
    "prediction_inception = Dense(class_count, activation='softmax', name='inception_predictions')(x_inception)\n",
    "\n",
    "# 4. Tạo model InceptionV3 hoàn chỉnh\n",
    "inception_model = Model(inputs=base_model_inception.input, outputs=prediction_inception, name='inception_custom')\n",
    "\n",
    "# 5. Callback để giảm learning rate và early stopping\n",
    "learning_rate_reduction_inception = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    factor=0.5,\n",
    "    min_lr=0.000001\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 6. Compile model với optimizer tốt hơn\n",
    "inception_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Sử dụng Adam thay vì RMSprop\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 7. In summary của model InceptionV3\n",
    "print(\"\\nCấu trúc mô hình InceptionV3:\")\n",
    "inception_model.summary()\n",
    "\n",
    "\n",
    "# 9. Huấn luyện model InceptionV3 với nhiều epochs hơn\n",
    "print(\"\\nBắt đầu huấn luyện mô hình InceptionV3...\")\n",
    "inception_history = inception_model.fit(\n",
    "    train_ds,\n",
    "    epochs=30,  # Tăng số epochs\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[learning_rate_reduction_inception, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:45:48.905105Z",
     "iopub.status.busy": "2025-05-10T04:45:48.904799Z",
     "iopub.status.idle": "2025-05-10T04:45:49.308409Z",
     "shell.execute_reply": "2025-05-10T04:45:49.307460Z",
     "shell.execute_reply.started": "2025-05-10T04:45:48.905079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_auc_loss(history, epochs):\n",
    "    # Lấy dữ liệu từ history\n",
    "    tloss = history.history['loss']  # Training loss\n",
    "    vloss = history.history['val_loss']  # Validation loss\n",
    "\n",
    "    # Kiểm tra và in ra số lượng phần tử trong các danh sách\n",
    "    print(f\"Number of epochs: {epochs}\")\n",
    "    print(f\"Training loss data points: {len(tloss)}\")\n",
    "    print(f\"Validation loss data points: {len(vloss)}\")\n",
    "\n",
    "    # Kiểm tra rằng số phần tử trong các list khớp với số epoch\n",
    "    if len(tloss) != epochs or len(vloss) != epochs:\n",
    "        print(\"Mismatch in number of epochs and data points. Adjusting to match.\")\n",
    "        epochs = min(len(tloss), len(vloss))  # Điều chỉnh lại epochs nếu có sự khác biệt\n",
    "\n",
    "    Epochs = range(1, epochs + 1)  # Tạo list Epochs có độ dài giống với loss\n",
    "\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "\n",
    "    # Vẽ biểu đồ Training Loss và Validation Loss\n",
    "    axes[0].plot(Epochs, tloss[:epochs], 'r', label='Training loss')  # Cắt phần tử phù hợp với số epoch\n",
    "    axes[0].plot(Epochs, vloss[:epochs], 'g', label='Validation loss')  # Cắt phần tử phù hợp với số epoch\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Vẽ biểu đồ Accuracy nếu có\n",
    "    if 'accuracy' in history.history:\n",
    "        tacc = history.history['accuracy']\n",
    "        vacc = history.history['val_accuracy']\n",
    "        axes[1].plot(Epochs, tacc[:epochs], 'r', label='Training accuracy')  # Cắt phần tử phù hợp với số epoch\n",
    "        axes[1].plot(Epochs, vacc[:epochs], 'g', label='Validation accuracy')  # Cắt phần tử phù hợp với số epoch\n",
    "        axes[1].set_title('Training and Validation accuracy')\n",
    "        axes[1].set_xlabel('Epochs')\n",
    "        axes[1].set_ylabel('accuracy')\n",
    "        axes[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sử dụng hàm với số epochs bạn đã dùng để huấn luyện\n",
    "plot_auc_loss(history, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:45:49.309867Z",
     "iopub.status.busy": "2025-05-10T04:45:49.309547Z",
     "iopub.status.idle": "2025-05-10T04:47:58.237715Z",
     "shell.execute_reply": "2025-05-10T04:47:58.236848Z",
     "shell.execute_reply.started": "2025-05-10T04:45:49.309840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predictor(test_ds):\n",
    "    y_pred, error_list, error_pred_list = [], [], []\n",
    "    y_true = test_ds.labels\n",
    "    classes = list(test_ds.class_indices.keys())\n",
    "    class_count = len(classes)\n",
    "    errors = 0\n",
    "    preds = tf.argmax(net.predict(test_ds), axis=1)\n",
    "    tests = len(preds)\n",
    "    for i in range(tests):\n",
    "        pred_index = preds[i]\n",
    "        true_index = test_ds.labels[i]\n",
    "        if pred_index != true_index:\n",
    "            errors += 1\n",
    "            file = test_ds.filenames[i]\n",
    "            error_list.append(file)\n",
    "            error_classes = classes[pred_index]\n",
    "            error_pred_list.append(error_classes)\n",
    "        y_pred.append(pred_index)\n",
    "\n",
    "    acc = (1 - errors / tests) * 100\n",
    "    msg = f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}%'\n",
    "    print(msg)\n",
    "    ypred = np.array(y_pred)\n",
    "    ytrue = np.array(y_true)\n",
    "    f1score = f1_score(ytrue, ypred, average='weighted') * 100\n",
    "\n",
    "    if class_count <= 30:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
    "        plt.xticks(np.arange(class_count) + .5, classes, rotation=90)\n",
    "        plt.yticks(np.arange(class_count) + .5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    clr = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
    "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "\n",
    "    return errors, tests, error_list, error_pred_list, f1score\n",
    "\n",
    "errors, tests, error_list, error_pred_list, f1score = predictor(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T04:47:58.240950Z",
     "iopub.status.busy": "2025-05-10T04:47:58.240566Z",
     "iopub.status.idle": "2025-05-10T04:47:58.394420Z",
     "shell.execute_reply": "2025-05-10T04:47:58.393585Z",
     "shell.execute_reply.started": "2025-05-10T04:47:58.240924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Kết hợp tất cả các DataFrame train, test, và validation\n",
    "all_data = pd.concat([train_df, test_df, valid_df])\n",
    "\n",
    "# Đếm số lượng mẫu cho mỗi lớp trong toàn bộ bộ dữ liệu\n",
    "class_counts_all = all_data['labels'].value_counts()\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts_all.plot(kind='bar', color='skyblue')\n",
    "plt.title('Number of Samples per Class (All datasets)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu model dưới dạng SavedModel\n",
    "saved_model_path = './model/vgg19_model'\n",
    "vgg19_model.save(saved_model_path, save_format='tf')\n",
    "print(f\"Model đã được lưu dưới dạng SavedModel tại: {saved_model_path}\")\n",
    "\n",
    "# Lưu model dưới dạng TFLite\n",
    "tflite_save_path = './model/vgg19_model.tflite'\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(vgg19_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(tflite_save_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"Model đã được lưu dưới dạng TFLite tại: {tflite_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải lại model đã lưu dưới dạng SavedModel\n",
    "loaded_model = tf.keras.models.load_model('./model/vgg19_model')\n",
    "\n",
    "# Kiểm tra cấu trúc model\n",
    "loaded_model.summary()\n",
    "\n",
    "# Chạy thử dự đoán trên một batch từ valid_ds\n",
    "for batch in valid_ds:\n",
    "    images, labels = batch\n",
    "    predictions = loaded_model.predict(images)\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"True Labels:\", labels)\n",
    "    break  # Chỉ chạy thử trên một batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    # Đảm bảo input_size và class_names đã được định nghĩa\n",
    "    input_size = (300, 300)  # Ví dụ với kích thước 600x600, thay đổi tùy theo mô hình của bạn\n",
    "    class_names = ['Black Rot', 'frog_eye_leaf_spot', 'healthy', 'powdery_mildew', 'rust', 'scab']  # Thay danh sách tên lớp vào đây\n",
    "\n",
    "    # Sử dụng mô hình SavedModel\n",
    "    loaded_model = tf.keras.models.load_model('./model/vgg19_model/')\n",
    "\n",
    "    # Dự đoán\n",
    "    def predict_with_saved_model(image_path):\n",
    "        # Tải ảnh và xử lý\n",
    "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=input_size)\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        #img_array = img_array / 255.0  # Chuẩn hóa giá trị ảnh\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Thêm batch dimension\n",
    "        \n",
    "        # Dự đoán\n",
    "        predictions = loaded_model.predict(img_array)\n",
    "        predicted_class = class_names[np.argmax(predictions[0])]  # Lớp có xác suất cao nhất\n",
    "        \n",
    "        # Trả về lớp dự đoán và xác suất dự đoán\n",
    "        return predicted_class, predictions[0]\n",
    "\n",
    "    # Ví dụ sử dụng hàm dự đoán\n",
    "    image_path = 'D:/pow.jfif'\n",
    "    predicted_class, prediction_probs = predict_with_saved_model(image_path)\n",
    "\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(\"Prediction Probabilities:\")\n",
    "    for class_name, prob in zip(class_names, prediction_probs):\n",
    "        print(f\"{class_name}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    # Đảm bảo input_size và class_names đã được định nghĩa\n",
    "    input_size = (300, 300)  # Ví dụ với kích thước 600x600, thay đổi tùy theo mô hình của bạn\n",
    "    class_names = ['Black Rot', 'frog_eye_leaf_spot', 'healthy', 'powdery_mildew', 'rust', 'scab']  # Thay danh sách tên lớp vào đây\n",
    "\n",
    "    # Sử dụng mô hình SavedModel\n",
    "    loaded_model = tf.keras.models.load_model('./model/vgg19_model/')\n",
    "\n",
    "    # Dự đoán\n",
    "    def predict_with_saved_model(image_path):\n",
    "        # Tải ảnh và xử lý\n",
    "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=input_size)\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        #img_array = img_array / 255.0  # Chuẩn hóa giá trị ảnh\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Thêm batch dimension\n",
    "        \n",
    "        # Dự đoán\n",
    "        predictions = loaded_model.predict(img_array)\n",
    "        predicted_class = class_names[np.argmax(predictions[0])]  # Lớp có xác suất cao nhất\n",
    "        \n",
    "        # Trả về lớp dự đoán và xác suất dự đoán\n",
    "        return predicted_class, predictions[0]\n",
    "\n",
    "    # Ví dụ sử dụng hàm dự đoán\n",
    "    image_path = 'D:/healthy.JPG'\n",
    "    predicted_class, prediction_probs = predict_with_saved_model(image_path)\n",
    "\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(\"Prediction Probabilities:\")\n",
    "    for class_name, prob in zip(class_names, prediction_probs):\n",
    "        print(f\"{class_name}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    # Đảm bảo input_size và class_names đã được định nghĩa\n",
    "    input_size = (300, 300)  # Ví dụ với kích thước 600x600, thay đổi tùy theo mô hình của bạn\n",
    "    class_names = ['Black Rot', 'frog_eye_leaf_spot', 'healthy', 'powdery_mildew', 'rust', 'scab']  # Thay danh sách tên lớp vào đây\n",
    "\n",
    "    # Sử dụng mô hình SavedModel\n",
    "    loaded_model = tf.keras.models.load_model('./model/vgg19_model/')\n",
    "\n",
    "    # Dự đoán\n",
    "    def predict_with_saved_model(image_path):\n",
    "        # Tải ảnh và xử lý\n",
    "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=input_size)\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        #img_array = img_array / 255.0  # Chuẩn hóa giá trị ảnh\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Thêm batch dimension\n",
    "        \n",
    "        # Dự đoán\n",
    "        predictions = loaded_model.predict(img_array)\n",
    "        predicted_class = class_names[np.argmax(predictions[0])]  # Lớp có xác suất cao nhất\n",
    "        \n",
    "        # Trả về lớp dự đoán và xác suất dự đoán\n",
    "        return predicted_class, predictions[0]\n",
    "\n",
    "    # Ví dụ sử dụng hàm dự đoán\n",
    "    image_path = 'D:/risat.jpg'\n",
    "    predicted_class, prediction_probs = predict_with_saved_model(image_path)\n",
    "\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(\"Prediction Probabilities:\")\n",
    "    for class_name, prob in zip(class_names, prediction_probs):\n",
    "        print(f\"{class_name}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Đường dẫn đến SavedModel đã lưu\n",
    "saved_model_path = './model/vgg19_model'\n",
    "\n",
    "# Đường dẫn thư mục để lưu model TensorFlow.js\n",
    "tfjs_model_dir = './model/modeljs'\n",
    "\n",
    "# Chuyển đổi model sang TensorFlow.js\n",
    "result = subprocess.run([\n",
    "    'tensorflowjs_converter',\n",
    "    '--input_format', 'tf_saved_model',\n",
    "    '--output_format', 'tfjs_graph_model',\n",
    "    '--saved_model_tags', 'serve',\n",
    "    saved_model_path,\n",
    "    tfjs_model_dir\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Lỗi trong quá trình chuyển đổi: {result.stderr}\")\n",
    "else:\n",
    "    print(f\"Model đã được chuyển đổi sang TensorFlow.js và lưu tại: {tfjs_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải lại model và kiểm tra các trọng số trong lớp cuối\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    model = tf.keras.models.load_model('./model/vgg19_model')\n",
    "\n",
    "    # Lấy ra tầng cuối cùng\n",
    "    final_layers = [layer for layer in model.layers if 'dense' in layer.name.lower() or 'res_mlp' in layer.name.lower()]\n",
    "    for layer in final_layers[-2:]:  # Xem 2 tầng cuối\n",
    "        weights = layer.get_weights()\n",
    "        print(f\"Layer: {layer.name}, Shape: {[w.shape for w in weights]}\")\n",
    "        # Kiểm tra bias có bị lệch không\n",
    "        if len(weights) > 1:  # Có bias\n",
    "            bias = weights[1]\n",
    "            print(f\"Bias values: {bias}\")\n",
    "            print(f\"Bias min: {np.min(bias)}, max: {np.max(bias)}, mean: {np.mean(bias)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2464177,
     "sourceId": 4176149,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7335914,
     "sourceId": 11688016,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
