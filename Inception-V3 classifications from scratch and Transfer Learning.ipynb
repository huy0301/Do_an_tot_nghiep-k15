{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzqFa5PqnF9D"
   },
   "source": [
    "Dataset credits: https://github.com/hsaleem1/NZDLPlantDisease-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7oHsaXjDiYS"
   },
   "source": [
    "Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44405,
     "status": "ok",
     "timestamp": 1714424861844,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "X41y4OBdDhlo",
    "outputId": "5cea6271-934d-4a2e-9f56-e42c6551ba43"
   },
   "outputs": [],
   "source": [
    "#Mount google drive to google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvaEAXHfEpmj"
   },
   "outputs": [],
   "source": [
    "data_path =\"/content/drive/MyDrive/Classroom/NewNZdataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdWOHsojgIS-"
   },
   "source": [
    "### Import all the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVUDSTzngIS_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Flatten\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1X4DyUKgITA"
   },
   "source": [
    "### Set all the Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftEAfA98gITB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 299  #---“input shape has to be `(224, 224, 3)” → INCEPTIONV3\n",
    "CHANNELS=3\n",
    "epochs =100\n",
    "classes =7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWTD-L2JgITB"
   },
   "source": [
    "### Import data into tensorflow dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-cjerLngITC"
   },
   "source": [
    "We will use image_dataset_from_directory api to load all images in tensorflow dataset: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6547,
     "status": "ok",
     "timestamp": 1714425842597,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "CLW-rXgPgITD",
    "outputId": "452d83e0-72fc-4aa1-9d96-0acedb8e2956"
   },
   "outputs": [],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_path,\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1714425851713,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "SLCkT0UHgITE",
    "outputId": "70fb5782-55b0-4f0b-bdd7-987abe1da63c"
   },
   "outputs": [],
   "source": [
    "#Print class names\n",
    "class_names = dataset.class_names\n",
    "\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAycvMLgdR7D"
   },
   "source": [
    "EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1714424893371,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "TKU0EjOudTnb",
    "outputId": "b255154e-1d2b-467e-ddb9-272f803eebbb"
   },
   "outputs": [],
   "source": [
    "#Total number of images ≈ Number of batches × Batch size\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 891274,
     "status": "ok",
     "timestamp": 1714425784629,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "LoRQQoP5cwPC",
    "outputId": "9771999a-5cac-4bee-94db-8da57c292de6"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of images per class\n",
    "class_counts = dict.fromkeys(class_names, 0)\n",
    "\n",
    "for images, labels in dataset:\n",
    "    for label in labels:\n",
    "        label = int(label.numpy())  # Convert to int if necessary\n",
    "        class_counts[class_names[label]] += 1\n",
    "\n",
    "# Plotting the class distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2rJZ7Qfl3yw"
   },
   "source": [
    "Count the Element in Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30531,
     "status": "ok",
     "timestamp": 1714425899797,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "7L3UVazJl4wo",
    "outputId": "f682f484-b698-4b72-f297-2133c6c8ee29"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through the dataset to count occurrences of each class\n",
    "for batch in dataset:\n",
    "    # Extract labels from the batch\n",
    "    labels = batch[1]\n",
    "\n",
    "    # Count occurrences of each label in this batch\n",
    "    for label in labels:\n",
    "        class_counts[label.numpy()] += 1\n",
    "\n",
    "# Display the counts for each class with class names\n",
    "print(\"Class counts:\")\n",
    "for class_index, count in class_counts.items():\n",
    "    class_name = class_names[class_index]\n",
    "    print(f\"{class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTP48DSbl8rZ"
   },
   "source": [
    "# Defining Class Weights\n",
    "\n",
    "There are many more observations of the Leaf spot class, and so this will have a larger impact on the overall value of the loss function than observations belonging to the other classes. the weight is going to be used for training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7Pf46UYmAIG"
   },
   "outputs": [],
   "source": [
    "# Example class counts (use your actual counts)\n",
    "class_counts = {\n",
    "    \"Healthy Leaf\": 2765,\n",
    "    \"Healthy Fruit\": 2282,\n",
    "    \"Black spot\": 1766,\n",
    "    \"European Canker\": 2147,\n",
    "    \"Mosaic virus\": 2636,\n",
    "    \"Leaf spot (scab)\": 3368,\n",
    "    \"Black rot\": 742,\n",
    "}\n",
    "\n",
    "# Total number of samples and unique classes\n",
    "total_samples = sum(class_counts.values())\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = {}\n",
    "for i, (class_name, count) in enumerate(class_counts.items()):\n",
    "    weight = total_samples / (num_classes * count)\n",
    "    class_weights[i] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1714419335728,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "rlXjiohXgITF",
    "outputId": "66e8d6cb-43c9-423c-e57f-27af2b4384c1"
   },
   "outputs": [],
   "source": [
    "#Check the shape of the first batch\n",
    "for image_batch, labels_batch in dataset.take(1):\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1714419337599,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "at1n3usfdepx",
    "outputId": "f832b3ad-eda9-4f40-e54d-656a5ef37dfb"
   },
   "outputs": [],
   "source": [
    "for image_batch in dataset.take(1):\n",
    "    print(image_batch[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VZzYYS1gITF"
   },
   "source": [
    "As you can see above, each element in the dataset is a tuple. First element is a batch of 32 elements of images. Second element is a batch of 32 elements of class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I9XTct3gITF"
   },
   "source": [
    "### Visualize some of the images from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "executionInfo": {
     "elapsed": 3307,
     "status": "ok",
     "timestamp": 1714419344811,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "YNV7KXF_gITG",
    "outputId": "7135f4f1-596d-415a-bf26-53ea78d45cfa"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for image_batch, labels_batch in dataset.take(1):\n",
    "    for i in range(12):\n",
    "        ax = plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels_batch[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKzlvMMbgITG"
   },
   "source": [
    "### Function to Split Dataset\n",
    "\n",
    "Dataset should be bifurcated into 3 subsets, namely:\n",
    "1. Training: Dataset to be used while training\n",
    "2. Validation: Dataset to be tested against while training\n",
    "3. Test: Dataset to be tested against after we trained a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqcLX2qjgITI"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset_partitions_tf(ds, train_split=0.7, val_split=0.2, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "\n",
    "    ds_size = len(ds)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "\n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "\n",
    "    train_ds = ds.take(train_size)\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgiPDrtpgITI"
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1714419369413,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "q_-5Z3YhgITI",
    "outputId": "46e5af48-2753-4692-95f3-52dc144b446c"
   },
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1714419371463,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "dhphTpfLgITI",
    "outputId": "bfeefeee-7bb4-4b1c-db5e-17568fb19277"
   },
   "outputs": [],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1714419374551,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "rClCwgg8gITJ",
    "outputId": "3e32cb61-7d9a-4b5d-e72f-6c3b794154e3"
   },
   "outputs": [],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HAoDg50gITJ"
   },
   "source": [
    "### Cache, Shuffle, and Prefetch the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0N8KHeQgITJ"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi2JsJ56gITJ"
   },
   "source": [
    "### Creating a Layer for Normalization\n",
    "Before we feed our images to network,  to improve model performance, we should normalize the image pixel value (keeping them in range 0 and 1 by dividing by 255).\n",
    "This should happen while training as well as inference. Hence we can add that as a layer in our Sequential Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjGfJ_FjmStK"
   },
   "outputs": [],
   "source": [
    "#normalization\n",
    "rescale = tf.keras.Sequential([\n",
    "layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-dyM6g9gITJ"
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLQ0EiGAgITQ"
   },
   "source": [
    "### Model Architecture\n",
    "We use a CNN coupled with a Softmax activation in the output layer. We also add the initial layers for normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkyeDObxHeI3"
   },
   "source": [
    " #  Training from Scratch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5qrx3G5H5OQ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Conv2D , MaxPool2D , Input , GlobalAveragePooling2D ,AveragePooling2D, Dense , Dropout ,Activation, Flatten , BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQMDVtorXLkh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrLpt9t0Yqm2"
   },
   "source": [
    "Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh_8KnaNHV4s"
   },
   "outputs": [],
   "source": [
    "def conv_with_Batch_Normalisation(prev_layer , nbr_kernels , filter_Size , strides =(1,1) , padding = 'same'):\n",
    "    x = Conv2D(filters=nbr_kernels, kernel_size = filter_Size, strides=strides , padding=padding)(prev_layer)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bk1q3-rRBMp3"
   },
   "outputs": [],
   "source": [
    "def StemBlock(prev_layer):\n",
    "    x = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 32, filter_Size=(3,3) , strides=(2,2))\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 32, filter_Size=(3,3))\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 64, filter_Size=(3,3))\n",
    "    x = MaxPool2D(pool_size=(3,3) , strides=(2,2)) (x)\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 80, filter_Size=(1,1))\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 192, filter_Size=(3,3))\n",
    "    x = MaxPool2D(pool_size=(3,3) , strides=(2,2)) (x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "au3I9pAlYjwz"
   },
   "source": [
    "Inception Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYpP4IlxBPG6"
   },
   "outputs": [],
   "source": [
    "def InceptionBlock_A(prev_layer  , nbr_kernels):\n",
    "\n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 64, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels=96, filter_Size=(3,3))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels=96, filter_Size=(3,3))\n",
    "\n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels=48, filter_Size=(1,1))\n",
    "    branch2 = conv_with_Batch_Normalisation(branch2, nbr_kernels=64, filter_Size=(3,3)) # may be 3*3\n",
    "\n",
    "    branch3 = AveragePooling2D(pool_size=(3,3) , strides=(1,1) , padding='same') (prev_layer)\n",
    "    branch3 = conv_with_Batch_Normalisation(branch3, nbr_kernels = nbr_kernels, filter_Size = (1,1))\n",
    "\n",
    "    branch4 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels=64, filter_Size=(1,1))\n",
    "\n",
    "    output = Concatenate(axis=3)([branch1, branch2, branch3, branch4])\n",
    "\n",
    "    return output\n",
    "\n",
    "def InceptionBlock_B(prev_layer , nbr_kernels):\n",
    "\n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = nbr_kernels, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = nbr_kernels, filter_Size = (7,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = nbr_kernels, filter_Size = (1,7))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = nbr_kernels, filter_Size = (7,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 192, filter_Size = (1,7))\n",
    "\n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = nbr_kernels, filter_Size = (1,1))\n",
    "    branch2 = conv_with_Batch_Normalisation(branch2, nbr_kernels = nbr_kernels, filter_Size = (1,7))\n",
    "    branch2 = conv_with_Batch_Normalisation(branch2, nbr_kernels = 192, filter_Size = (7,1))\n",
    "\n",
    "    branch3 = AveragePooling2D(pool_size=(3,3) , strides=(1,1) , padding ='same') (prev_layer)\n",
    "    branch3 = conv_with_Batch_Normalisation(branch3, nbr_kernels = 192, filter_Size = (1,1))\n",
    "\n",
    "    branch4 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 192, filter_Size = (1,1))\n",
    "\n",
    "    output = Concatenate(axis=3)([branch1, branch2, branch3, branch4])\n",
    "\n",
    "    return output\n",
    "\n",
    "def InceptionBlock_C(prev_layer):\n",
    "\n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 448, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 384, filter_Size = (3,3))\n",
    "    branch1_1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 384, filter_Size = (1,3))\n",
    "    branch1_2 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 384, filter_Size = (3,1))\n",
    "    branch1 = Concatenate(axis=3)([branch1_1, branch1_2])\n",
    "\n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 384, filter_Size = (1,1))\n",
    "    branch2_1 = conv_with_Batch_Normalisation(branch2, nbr_kernels = 384, filter_Size = (1,3))\n",
    "    branch2_2 = conv_with_Batch_Normalisation(branch2, nbr_kernels = 384, filter_Size = (3,1))\n",
    "    branch2 = Concatenate(axis=3)([branch2_1, branch2_2])\n",
    "\n",
    "    branch3 = AveragePooling2D(pool_size=(3,3) , strides=(1,1) , padding='same')(prev_layer)\n",
    "    branch3 = conv_with_Batch_Normalisation(branch3, nbr_kernels = 192, filter_Size = (1,1))\n",
    "\n",
    "    branch4 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 320, filter_Size = (1,1))\n",
    "\n",
    "    output = Concatenate(axis=3)([branch1, branch2, branch3, branch4])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znGDEoDxYiHB"
   },
   "source": [
    "Reduction Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01KKdvbbBV-i"
   },
   "outputs": [],
   "source": [
    "def ReductionBlock_A(prev_layer):\n",
    "\n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 64, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 96, filter_Size = (3,3))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 96, filter_Size = (3,3) , strides=(2,2) ) #, padding='valid'\n",
    "\n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 384, filter_Size=(3,3) , strides=(2,2) )\n",
    "\n",
    "    branch3 = MaxPool2D(pool_size=(3,3) , strides=(2,2) , padding='same')(prev_layer)\n",
    "\n",
    "    output = Concatenate(axis=3)([branch1, branch2, branch3])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def ReductionBlock_B(prev_layer):\n",
    "\n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 192, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 192, filter_Size = (1,7))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 192, filter_Size = (7,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 192, filter_Size = (3,3) , strides=(2,2) , padding = 'valid')\n",
    "\n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 192, filter_Size = (1,1) )\n",
    "    branch2 = conv_with_Batch_Normalisation(branch2, nbr_kernels = 320, filter_Size = (3,3) , strides=(2,2) , padding='valid' )\n",
    "\n",
    "    branch3 = MaxPool2D(pool_size=(3,3) , strides=(2,2) )(prev_layer)\n",
    "\n",
    "    output = Concatenate(axis=3)([branch1, branch2, branch3])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdyGJGTHYyG5"
   },
   "source": [
    "Auxiliary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f38BiapsBvcq"
   },
   "outputs": [],
   "source": [
    "def auxiliary_classifier(prev_Layer):\n",
    "    x = AveragePooling2D(pool_size=(5,5) , strides=(3,3)) (prev_Layer)\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 128, filter_Size = (1,1))\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units = 768, activation='relu') (x)\n",
    "    x = Dropout(rate = 0.2) (x)\n",
    "    x = Dense(units = 1000, activation='softmax') (x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d70zKk3Y4xV"
   },
   "source": [
    "The Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AVdS4XWBzzR"
   },
   "outputs": [],
   "source": [
    "def inceptionV3():\n",
    "\n",
    "    input_layer = Input(shape=(299 , 299 , 3))\n",
    "    x = rescale(input_layer)\n",
    "\n",
    "    x = StemBlock(x)\n",
    "\n",
    "    x = InceptionBlock_A(prev_layer = x ,nbr_kernels = 32)\n",
    "    x = InceptionBlock_A(prev_layer = x ,nbr_kernels = 64)\n",
    "    x = InceptionBlock_A(prev_layer = x ,nbr_kernels = 64)\n",
    "\n",
    "    x = ReductionBlock_A(prev_layer = x )\n",
    "\n",
    "    x = InceptionBlock_B(prev_layer = x  , nbr_kernels = 128)\n",
    "    x = InceptionBlock_B(prev_layer = x , nbr_kernels = 160)\n",
    "    x = InceptionBlock_B(prev_layer = x , nbr_kernels = 160)\n",
    "    x = InceptionBlock_B(prev_layer = x , nbr_kernels = 192)\n",
    "\n",
    "    Aux = auxiliary_classifier(prev_Layer = x)\n",
    "\n",
    "    x = ReductionBlock_B(prev_layer = x)\n",
    "\n",
    "    x = InceptionBlock_C(prev_layer = x)\n",
    "    x = InceptionBlock_C(prev_layer = x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(units=2048, activation='relu') (x)\n",
    "    x = Dropout(rate = 0.2) (x)\n",
    "    x = Dense(units=classes, activation='softmax') (x)\n",
    "\n",
    "    model = Model(inputs = input_layer , outputs = x , name = 'Inception-V3')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dF9WawlnEqyB"
   },
   "outputs": [],
   "source": [
    "# Instantiate the function\n",
    "model = inceptionV3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0WcRtxN2p3Z"
   },
   "source": [
    "# Defining Callbacks\n",
    "\n",
    "A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MT5KFK4vSSMo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Callback to reduce the learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=np.sqrt(0.1),  # Factor to reduce learning rate\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Callback to stop training when validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs to wait before stopping\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vqHgNU_2852"
   },
   "source": [
    "Defining the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RovY6fbz3D1J"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5CiCzdX_O59"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1444276,
     "status": "ok",
     "timestamp": 1714421954277,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "K-pbZRWL_YzE",
    "outputId": "a3916399-edbb-4511-b606-420d377d3a37"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=2,\n",
    "    epochs=epochs,\n",
    "    class_weight = class_weights,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI4zKxyznjwU"
   },
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26705,
     "status": "ok",
     "timestamp": 1714422328923,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "ub0kniA2_fXe",
    "outputId": "f72b2b86-9825-4720-aade-7d6586f93847"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzMC2p3knnO7"
   },
   "source": [
    "Make Prediction on a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 6834,
     "status": "ok",
     "timestamp": 1714422349027,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "wt_mCrpfIxJA",
    "outputId": "718f662a-5182-4f6e-ba9f-69204197c281"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for images_batch, labels_batch in test_ds.take(1):\n",
    "\n",
    "    first_image = images_batch[0].numpy().astype('uint8')\n",
    "    first_label = labels_batch[0].numpy()\n",
    "\n",
    "    print(\"first image to predict\")\n",
    "    plt.imshow(first_image)\n",
    "    print(\"actual label:\",class_names[first_label])\n",
    "\n",
    "    batch_prediction = model.predict(images_batch)\n",
    "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wRqz8WV4Lrb"
   },
   "source": [
    "Plotting training and validation accuracy, loss and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUHiYiLf4GFW"
   },
   "outputs": [],
   "source": [
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "learning_rate = history.history['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1714422349647,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "1y25TYRO4Iin",
    "outputId": "9076daca-57ca-43f6-da0f-13548b1efe04"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 10))\n",
    "\n",
    "ax[0].set_title('Training Accuracy vs. Epochs')\n",
    "ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')\n",
    "ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].set_title('Training/Validation Loss vs. Epochs')\n",
    "ax[1].plot(train_loss, 'o-', label='Train Loss')\n",
    "ax[1].plot(val_loss, 'o-', label='Validation Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "ax[2].set_title('Learning Rate vs. Epochs')\n",
    "ax[2].plot(learning_rate, 'o-', label='Learning Rate')\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('Loss')\n",
    "ax[2].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyyOD1d7nwBk"
   },
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7275,
     "status": "ok",
     "timestamp": 1714422356912,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "J_rXuKltnzJI",
    "outputId": "ab133a85-d055-4a63-a287-c8473e864f3c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Get true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    y_true.extend(labels.numpy())  # Collect true labels\n",
    "    predictions = model.predict(images)  # Get model predictions\n",
    "    y_pred.extend(tf.argmax(predictions, axis=1).numpy())  # Get predicted classes\n",
    "\n",
    "# Confusion Matrix with class names\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cf_matrix, annot=True, cmap=\"Blues\", fmt=\"g\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF3vQl0sn3Nq"
   },
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1714422356913,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "vwDjiKnHn7bK",
    "outputId": "d701dd1c-6fee-476f-aa0e-e39d117132f0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, zero_division=1)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFrm8-YrqIUu"
   },
   "source": [
    "ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5020,
     "status": "ok",
     "timestamp": 1714422361886,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "bNC3joEZqLIK",
    "outputId": "4df6a3bd-c164-4689-ac56-182d41073a91"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_true = []\n",
    "y_pred_probs = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    y_true.extend(labels.numpy())  # Collect true labels\n",
    "    predictions = model.predict(images)  # Get model predictions (probabilities)\n",
    "    y_pred_probs.extend(predictions)  # Collect predicted probabilities\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "y_true = np.array(y_true)  # Ensure true labels are arrays\n",
    "y_pred_probs = np.array(y_pred_probs)  # Ensure predicted probabilities are arrays\n",
    "\n",
    "# Binarize the true labels to get a one-hot encoded representation\n",
    "y_true_bin = label_binarize(y_true, classes=list(range(20)))\n",
    "\n",
    "# Initialize ROC and AUC data\n",
    "fpr = dict()  # False positive rates\n",
    "tpr = dict()  # True positive rates\n",
    "roc_auc = dict()  # AUC for each class\n",
    "\n",
    "# Compute ROC and AUC for each class\n",
    "for i in range(classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])  # ROC data for each class\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])  # AUC for each class\n",
    "\n",
    "# Compute macro-average AUC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "# Compute mean true positive rates\n",
    "for i in range(classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= classes  # Normalize by the number of classes\n",
    "macro_auc = auc(all_fpr, mean_tpr)  # Macro-average AUC\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {class_names[i]} (AUC = {roc_auc[i]:.2f})')  # Label with class names and AUC\n",
    "\n",
    "# Plot the macro-average ROC curve\n",
    "plt.plot(all_fpr, mean_tpr, color='k', linestyle='--', label=f'Macro-average ROC (AUC = {macro_auc:.2f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-Class ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71yZx47wHZe3"
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYqObFiKbLYr"
   },
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "from keras.layers import Input, Rescaling\n",
    "\n",
    "input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))  # Adjust to your desired input shape\n",
    "x = rescale(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5616,
     "status": "ok",
     "timestamp": 1714425965123,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "_kh2Lb10b28q",
    "outputId": "69ed6221-dd14-4809-92bd-2cf1a2623f47"
   },
   "outputs": [],
   "source": [
    "# Initialize the InceptionV3 model without 'input_tensor'\n",
    "inception = InceptionV3(input_tensor=x, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doaTpnyzxXQD"
   },
   "outputs": [],
   "source": [
    "# Freeze the layers of InceptionV3\n",
    "for layer in inception.layers:\n",
    "  layer.trainable =False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2421,
     "status": "ok",
     "timestamp": 1714425976118,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "KoLy6Xqfh9bS",
    "outputId": "e9bc6451-614f-4250-b8b7-bfdc88df91d3"
   },
   "outputs": [],
   "source": [
    "x = GlobalAveragePooling2D()(inception.output)\n",
    "prediction = Dense(classes, activation ='softmax')(x)\n",
    "#create a model object\n",
    "model_tl =Model(inputs =inception.input, outputs =prediction)\n",
    "#View the structure of the model\n",
    "model_tl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzhCefafsEzj"
   },
   "outputs": [],
   "source": [
    "#Define Callback\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Callback to reduce the learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=np.sqrt(0.1),  # Factor to reduce learning rate\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Callback to stop training when validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs to wait before stopping\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8fAcBlLsQHn"
   },
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-dxtYoIgITR"
   },
   "source": [
    "### Compiling the Model\n",
    "We use `adam` Optimizer, `SparseCategoricalCrossentropy` for losses, `accuracy` as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-L2IdmLgITR"
   },
   "outputs": [],
   "source": [
    "model_tl.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1960144,
     "status": "ok",
     "timestamp": 1714428005665,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "3uYTIqvrgITR",
    "outputId": "441c96b4-8117-405a-dbf7-b9d112ae7503",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model_tl.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=2,\n",
    "    epochs=epochs,\n",
    "    class_weight = class_weights,\n",
    "    callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIDQEHOdsfoK"
   },
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31210,
     "status": "ok",
     "timestamp": 1714428075540,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "8d7XVY5bgITR",
    "outputId": "a5c0caee-474b-4c66-8046-aa0bd3d1b529"
   },
   "outputs": [],
   "source": [
    "scores = model_tl.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBK-RuJ1gITU"
   },
   "source": [
    "### Run prediction on a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 2850,
     "status": "ok",
     "timestamp": 1714428124697,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "R4ipE6Cfsy6q",
    "outputId": "47b4760b-9fd5-41c7-b442-3ad6cd4fea20"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for images_batch, labels_batch in test_ds.take(1):\n",
    "\n",
    "    first_image = images_batch[0].numpy().astype('uint8')\n",
    "    first_label = labels_batch[0].numpy()\n",
    "\n",
    "    print(\"first image to predict\")\n",
    "    plt.imshow(first_image)\n",
    "    print(\"actual label:\",class_names[first_label])\n",
    "\n",
    "    batch_prediction = model_tl.predict(images_batch)\n",
    "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9aEjdqIgITT"
   },
   "source": [
    "# Plotting the accuracy, loss and the epoch curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKk46PIIgITU"
   },
   "outputs": [],
   "source": [
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "learning_rate = history.history['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1714428125470,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "qoZEpxBHgITU",
    "outputId": "fd84085d-1924-45b2-99ae-a544b0c33add"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 10))\n",
    "\n",
    "ax[0].set_title('Training Accuracy vs. Epochs')\n",
    "ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')\n",
    "ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].set_title('Training/Validation Loss vs. Epochs')\n",
    "ax[1].plot(train_loss, 'o-', label='Train Loss')\n",
    "ax[1].plot(val_loss, 'o-', label='Validation Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "ax[2].set_title('Learning Rate vs. Epochs')\n",
    "ax[2].plot(learning_rate, 'o-', label='Learning Rate')\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('Loss')\n",
    "ax[2].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNv6n2__tCTb"
   },
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6077,
     "status": "ok",
     "timestamp": 1714428131531,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "sddsNHyntift",
    "outputId": "5aa652b1-54a1-48b8-dee1-0ab7c938f091"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Get true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    y_true.extend(labels.numpy())  # Collect true labels\n",
    "    predictions = model_tl.predict(images)  # Get model predictions\n",
    "    y_pred.extend(tf.argmax(predictions, axis=1).numpy())  # Get predicted classes\n",
    "\n",
    "# Confusion Matrix with class names\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cf_matrix, annot=True, cmap=\"Blues\", fmt=\"g\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2loSCWctErd"
   },
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1714428131531,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "5LsCAFBvtm6-",
    "outputId": "cfb54846-2408-4421-d388-258dde7085ec"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, zero_division=1)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YAWbLaatJD8"
   },
   "source": [
    "ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4298,
     "status": "ok",
     "timestamp": 1714428135802,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "iOuYlr4StqYZ",
    "outputId": "100f4e9e-2c97-4b43-c4ef-66ea5b031179"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_true = []\n",
    "y_pred_probs = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    y_true.extend(labels.numpy())  # Collect true labels\n",
    "    predictions = model_tl.predict(images)  # Get model predictions (probabilities)\n",
    "    y_pred_probs.extend(predictions)  # Collect predicted probabilities\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "y_true = np.array(y_true)  # Ensure true labels are arrays\n",
    "y_pred_probs = np.array(y_pred_probs)  # Ensure predicted probabilities are arrays\n",
    "\n",
    "# Binarize the true labels to get a one-hot encoded representation\n",
    "y_true_bin = label_binarize(y_true, classes=list(range(20)))\n",
    "\n",
    "# Initialize ROC and AUC data\n",
    "fpr = dict()  # False positive rates\n",
    "tpr = dict()  # True positive rates\n",
    "roc_auc = dict()  # AUC for each class\n",
    "\n",
    "# Compute ROC and AUC for each class\n",
    "for i in range(classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])  # ROC data for each class\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])  # AUC for each class\n",
    "\n",
    "# Compute macro-average AUC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "# Compute mean true positive rates\n",
    "for i in range(classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= classes  # Normalize by the number of classes\n",
    "macro_auc = auc(all_fpr, mean_tpr)  # Macro-average AUC\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {class_names[i]} (AUC = {roc_auc[i]:.2f})')  # Label with class names and AUC\n",
    "\n",
    "# Plot the macro-average ROC curve\n",
    "plt.plot(all_fpr, mean_tpr, color='k', linestyle='--', label=f'Macro-average ROC (AUC = {macro_auc:.2f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-Class ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgNuiB8AtpxR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erD8QV_BgITU"
   },
   "source": [
    "### Write a function for inference for the better model - from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GzovfAGgITU"
   },
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv0WGKeOgITV"
   },
   "source": [
    "**Now run inference on few sample images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1K7ZEowqVf0cKvrz2ZikMCjuWnn7uLeAa"
    },
    "executionInfo": {
     "elapsed": 11505,
     "status": "ok",
     "timestamp": 1714428147284,
     "user": {
      "displayName": "olalekan opeyemi",
      "userId": "00855638922048451526"
     },
     "user_tz": -60
    },
    "id": "qgwwxTJAgITV",
    "outputId": "45509dbc-2da1-45dd-f6cd-8711c4b29740"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "\n",
    "        predicted_class, confidence = predict(model_tl, images[i].numpy())\n",
    "        actual_class = class_names[labels[i]]\n",
    "\n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "\n",
    "        plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM+q88y4fwtTW9PaztKwhkB",
   "cell_execution_strategy": "setup",
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
